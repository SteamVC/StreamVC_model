experiment:
  name: streamvc_phase1_ema_gan
  seed: 1234

data:
  sample_rate: 16000
  frame_ms: 20.0
  lookahead_frames: 3
  cache_dir: data/cache
  datasets:
    - name: libri_tts
      root: data/libritts
      language: en
      metadata: data/libritts/metadata.jsonl
      weight: 1.0
  sample_length_sec: 1.28
  reference_length_sec: 1.0
  hubert_cache: data/cache/hubert
  yin_cache: data/cache/yin

model:
  num_hubert_labels: 100
  content_encoder:
    channels: 64
    latent_dim: 64
    num_layers: 6
    kernel_size: 7
    stride_schedule: [2, 2, 2]
    dilation_growth: 2
    dropout: 0.1
  speaker_encoder:
    channels: 32
    latent_dim: 64
    num_layers: 6
    kernel_size: 5
    stride_schedule: [2, 2]
    dilation_growth: 2
  decoder:
    channels: 40
    latent_dim: 64
    upsample_factors: [2, 2, 2]
    kernel_size: 7
    num_residual_blocks: 3
    rvq:
      num_quantizers: 8
      codebook_size: 1024
      dims: 40
      commitment_cost: 0.5
      progressive_steps: 2000  # Activate 1 quantizer every 2K steps
      use_cosine_sim: false
      use_ste_fix: true
      # Phase 1-EMA: Continue using EMA-based codebook update
      use_ema: true
      ema_decay: 0.99
      dead_threshold: 100

training:
  batch_size: 8  # Keep original batch size (user requested)
  num_steps: 15000  # Extended for GAN training
  optimizer:
    type: adamw
    lr: 0.0001
    betas: [0.9, 0.99]
    weight_decay: 0.0
  scheduler:
    type: cosine
    warmup_steps: 2000
    eta_min: 0.000001
  losses:
    content_ce_weight: 1.0
    stft_weight: 1.0
    l1_weight: 10.0
    adversarial_weight: 4.0      # Phase 1-EMA-GAN: Enable adversarial loss
    feature_matching_weight: 2.0 # Phase 1-EMA-GAN: Enable feature matching
    rms_weight: 1.0              # Phase 1-EMA-GAN: Increase RMS weight (10x)
    multiband_rms_weight: 0.5    # Phase 1-EMA-GAN: Increase multiband RMS (10x)
  log_interval: 100
  eval_interval: 1000
  ckpt_interval: 2000
  output_dir: runs/streamvc_phase1_ema_gan
  # GAN warm-up strategy (defined in trainer)
  gan_warmup_steps: 2000   # Generator-only pretraining for first 2K steps
  gan_rampup_steps: 2000   # Linear ramp-up from step 2K to 4K

inference:
  chunk_ms: 20.0
  quantize: false
  export:
    tflite: false
    onnx: false
