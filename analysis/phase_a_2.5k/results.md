# Phase A @ 2.5K Step - Analysis Report

**Date**: 2025-11-07
**Checkpoint**: runs/streamvc_phase_a_scale_fix/checkpoints/step_2000.pt (interpolated to 2.5K)

## Executive Summary

ðŸš¨ **CRITICAL FAILURE**: Phase A exhibits the **same amplitude collapse** as Phase 2A despite all mitigation efforts.

- âœ… out_proj norm: Stable (-5.6%, target <10%)
- âœ… Perplexity Q0: 8.55 (target â‰¥8)
- âŒ **Audio RMS: 0.0037 (95% below target 0.075)**
- âŒ RMS Lossé™ä¸‹ (-64%) but å®Ÿéš›ã®æŒ¯å¹…ã¯å´©å£Š

## Metrics Comparison

### Loss Trends (0â†’2.5K step)

| Metric | 0-500 | 1K-1.5K | 2K-2.5K | Change |
|--------|-------|---------|---------|--------|
| **Total Loss** | 12.03 | 7.62 | 8.33 | -30.8% |
| STFT Loss | 4.11 | 1.27 | 1.20 | -70.8% |
| L1 Loss | 0.203 | 0.057 | 0.052 | -74.5% |
| RVQ Loss | 1.15 | 1.24 | 2.16 | **+87.5%** âš ï¸ |
| **RMS Loss** | 0.173 | 0.056 | 0.061 | -64.4% |
| Multi-band RMS | 0.092 | 0.023 | 0.021 | -76.7% |

### RVQ Diagnostics

| Metric | 0-500 | 1K-1.5K | 2K-2.5K | Target | Status |
|--------|-------|---------|---------|--------|--------|
| Perplexity Q0 | 25.6 | 11.1 | 8.6 | â‰¥8 | âœ“ Good |
| Perplexity Q1 | - | - | 16.2 | â‰¥8 | âœ“ Good |

### Critical Parameters

| Parameter | Initial | @ 2.5K | Change | Target | Status |
|-----------|---------|--------|--------|--------|--------|
| **out_proj W norm** | 0.564 | 0.536 | -5.0% | <10% | âœ“ **Stable** |
| out_proj B norm | 0.135 | 0.109 | -19.3% | <10% | âš ï¸ Degrading |
| final_conv norm | 3.659 | 3.630 | -0.8% | Stable | âœ“ Stable |
| **Audio RMS** | 0.231 | **0.0037** | **-97.9%** | 0.075Â±20% | âŒ **CRITICAL** |

## Problem Analysis

### 1. RMS Loss is Working BUT Ineffective

**Paradox**:
- RMS Loss: 0.173 â†’ 0.061 (-64%, converging)
- Multi-band RMS: 0.092 â†’ 0.021 (-77%, converging)
- **Actual Audio RMS: 0.231 â†’ 0.0037 (-98%, collapsing)**

**Interpretation**:
- Lossé–¢æ•°ã¯ã€Œå°ã•ãªæŒ¯å¹…ã§ã‚‚RMSã‚’åˆã‚ã›ã‚‰ã‚Œã‚‹ã€ã¨å­¦ç¿’
- Target RMSãŒ0.075ã§ã‚‚ã€pred/targetãŒä¸¡æ–¹ã‚¼ãƒ­ã«è¿‘ã¥ã‘ã° loss=0
- **Scale-invariant lossã®æ ¹æœ¬çš„å•é¡Œ**

### 2. RVQ Loss Increase (+87.5%)

- Phase 2Aã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³: RVQ lossâ†‘ = Code collapseé€²è¡Œ
- Perplexityã¯8.6ã§è‰¯å¥½ãªã®ã«ã€ãªãœRVQ lossãŒå¢—åŠ ï¼Ÿ
- **ä»®èª¬**: Encoderå‡ºåŠ›ã¨Codebookã®åº§æ¨™ãŒä¹–é›¢
  - Pre-RVQæ­£è¦åŒ–ã§std=1ã«å›ºå®š
  - ã—ã‹ã—Codebookã¯å­¦ç¿’ä¸­ã«å¤‰åŒ–
  - â†’ Commitment losså¢—åŠ 

### 3. out_proj vs final_conv

**Good news**:
- out_proj W: -5.0% (Phase 2A: -73%)
- final_conv: -0.8% (ã»ã¼å®‰å®š)

**Bad news**:
- out_proj B: -19.3% (weight_decay=0ã§ã‚‚æ¸›å°‘)
- Biasã ã‘ãŒæ¸›å°‘ â†’ å…¨ä½“ã®DCã‚ªãƒ•ã‚»ãƒƒãƒˆãŒæ¸›å°‘ â†’ æŒ¯å¹…å´©å£Š

### 4. STE Fix is Not Fixing

å®Ÿè£…ã—ãŸ STE consistency fix:
```python
# Phase A implementation
embeds_st = residual + (embeds - residual).detach()
embeds_scaled = embeds_st * post_scale + post_bias
```

**ã—ã‹ã—**:
- ä¾ç„¶ã¨ã—ã¦æŒ¯å¹…å´©å£ŠãŒç™ºç”Ÿ
- `post_scale`/`post_bias`ãŒé©åˆ‡ã«æ©Ÿèƒ½ã—ã¦ã„ãªã„å¯èƒ½æ€§
- ã¾ãŸã¯ `final_conv` ã®å¾Œå‡¦ç†ãŒä¸è¶³

## Root Cause Hypothesis

### Why RMS Supervision Failed

**Target matching ã®å•é¡Œ**:
```python
# frame_rms_loss ã®å®Ÿè£…
pred_rms = sqrt(pred^2.mean())   # Pred RMSã‚’è¨ˆç®—
target_rms = sqrt(target^2.mean()) # Target RMSã‚’è¨ˆç®—
loss = L1(pred_rms, target_rms)    # L1 loss
```

**å•é¡Œç‚¹**:
- PredãŒå…¨ä½“çš„ã«ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã—ã¦ã‚‚ã€**ç›¸å¯¾çš„ãªå½¢çŠ¶ãŒä¿ãŸã‚Œã‚Œã°lossã¯ä¸‹ãŒã‚‹**
- ä¾‹: pred = target Ã— 0.01 ã§ã‚‚ã€frame-wise RMSã®**æ¯”çŽ‡**ãŒåˆãˆã°losså°

**å¿…è¦ãªä¿®æ­£**:
- çµ¶å¯¾çš„ãªRMSã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨­å®š
- ã¾ãŸã¯ã€RMS **ratio** ã‚’1.0ã«å›ºå®šã™ã‚‹åˆ¶ç´„

### Why out_proj Bias Collapsed

**weight_decay=0ã§ã‚‚æ¸›å°‘**:
- Gradient descentè‡ªä½“ãŒbiasã‚’ç¸®å°æ–¹å‘ã«èª˜å°Ž
- ç†ç”±: å°ã•ãªå‡ºåŠ› = å°ã•ãªloss (STFT/L1ãŒ scale-invariant)
- **Explicit regularization** ãŒå¿…è¦ï¼ˆæ¸›å°‘ã‚’ç©æ¥µçš„ã«é˜²ãï¼‰

## Recommendations

### Immediate Action (ç·Šæ€¥ä¿®æ­£)

**Option 1: RMS Loss ã‚’çµ¶å¯¾å€¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¤‰æ›´**
```python
def absolute_rms_loss(pred, target_rms=0.075):
    pred_rms = torch.sqrt((pred ** 2).mean() + 1e-8)
    return F.mse_loss(pred_rms, torch.tensor(target_rms, device=pred.device))
```

**Option 2: Scale Anchor Loss (ã‚¹ã‚±ãƒ¼ãƒ«å›ºå®š)**
```python
def scale_anchor_loss(pred, target):
    # Force pred and target to have same RMS
    pred_rms = torch.sqrt((pred ** 2).mean() + 1e-8)
    target_rms = torch.sqrt((target ** 2).mean() + 1e-8)
    scale_ratio = target_rms / (pred_rms + 1e-8)

    # Penalize deviation from scale_ratio=1.0
    return F.mse_loss(scale_ratio, torch.ones_like(scale_ratio))
```

**Option 3: out_proj/final_conv ã« Norm Regularization**
```python
# In train_step
out_proj_norm_target = 0.5  # Initial norm
norm_reg = F.mse_loss(
    out_proj_weight_norm,
    torch.tensor(out_proj_norm_target, device=device)
)
total_loss += 0.01 * norm_reg
```

### Next Steps

1. **Kill current training** (æŒ¯å¹…å´©å£ŠãŒé€²è¡Œä¸­)
2. **Phase A-v2 å®Ÿè£…**:
   - Absolute RMS target loss
   - Scale anchor loss
   - Norm regularization
3. **2K stepã§æ—©æœŸæ¤œè¨¼**
4. æˆåŠŸãªã‚‰5Kâ†’10Kã¸ç¶™ç¶š

## Phase Comparison

| Phase | @ 2.5K RMS | @ 2.5K out_proj | @ 2.5K Perplexity | Verdict |
|-------|-----------|-----------------|-------------------|---------|
| Phase 1 | ~0.080 | 0.083 | 5-6 | ãƒ¢ã‚¹ã‚­ãƒ¼ãƒˆãƒ¼ãƒ³ã€æŒ¯å¹…OK |
| Phase 2A | **0.004** | **0.022 (-73%)** | 16â†’8 | æŒ¯å¹…å´©å£Šã€perpä¸€æ™‚æ”¹å–„ |
| **Phase A** | **0.0037** | 0.536 (-5%) | 8.6 | **æŒ¯å¹…å´©å£Šã€out_projå®‰å®šã¯ä¸ååˆ†** |

## Conclusion

**Phase A ã®è©•ä¾¡: FAILED**

- âœ… STE fix: å®Ÿè£…æ¸ˆã¿ã ãŒåŠ¹æžœãªã—
- âœ… out_proj weight: å®‰å®š (-5%)
- âŒ **RMS supervision: å®Ÿè£…æ¸ˆã¿ã ãŒã‚¹ã‚±ãƒ¼ãƒ«å´©å£Šã‚’é˜²ã’ãš**
- âŒ **Audio RMS: Phase 2Aã¨åŒãƒ¬ãƒ™ãƒ«ã®å´©å£Š (98%æ¸›)**

**æ ¹æœ¬åŽŸå› **:
- Scale-invariant loss (STFT, L1, RMS) ã®æ§‹é€ çš„å•é¡Œ
- "å°ã•ãå‡ºåŠ›ã™ã‚Œã°å‹ã¡" ã®æŠœã‘é“ãŒä¾ç„¶ã¨ã—ã¦å­˜åœ¨

**Next Action**:
- Phase A-v2 ã§çµ¶å¯¾å€¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ + Normæ­£å‰‡åŒ–ã‚’å°Žå…¥
- ã¾ãŸã¯ Phase 1ã«æˆ»ã£ã¦GANè¿½åŠ ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å•é¡Œã‚’åˆ¤åˆ¥å™¨ã«ä»»ã›ã‚‹ï¼‰
